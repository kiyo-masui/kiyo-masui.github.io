<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Kiyoshi Masui - Random</title>
<meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8" />
<meta name="description" content="Kiyoshi Masui's personal webpage.  Kiyo doesn't actually keep a blog." />
<link rel="stylesheet" type="text/css" href="css/layout.css" media="screen, projection, tv " />
<link rel="stylesheet" type="text/css" href="css/html.css" media="screen, projection, tv " />
<script type="text/javascript"> 
<!-- 
if (document.getElementById) { window.onload = swap };
function swap() {
var numimages=13;
rndimg = new Array("images/headers/hdr1.jpg", "images/headers/hdr2.jpg",
"images/headers/hdr3.jpg", "images/headers/hdr4.jpg", "images/headers/hdr5.jpg",
"images/headers/hdr6.jpg", "images/headers/hdr7.jpg", "images/headers/hdr8.jpg",
"images/headers/hdr9.jpg", "images/headers/hdr10.jpg", "images/headers/hdr11.jpg",
"images/headers/hdr12.jpg", "images/headers/hdr13.jpg"
); 
x=(Math.floor(Math.random()*numimages));
randomimage=(rndimg[x]);
document.getElementById("headerImg").style.backgroundImage = "url("+ randomimage +")"; 
}
//--> 
</script>
</head>
<body>
<!-- #content: holds all except site footer - causes footer to stick to bottom -->
<div id="content">
  <!-- #header: holds the logo and top links -->
  <div id="header" class="width"> <span><font size="4" color='white'>Kiyoshi Masui</font></span>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="research.html">Research</a></li>
      <li><a href="about.html">About Me</a></li>
      <li><a href="random.html" class="last"><font color='white'>Random</font></a></li>
    </ul>
  </div>
  <!-- #header end -->
  <!-- #headerImg: holds the main header image or flash -->
  <div id="headerImg" class="width"></div>
  <!-- #menu: the main large box site menu -->
  <div id="menu" class="width">
    <ul>
	  <li><br/><br/></li>
	</ul>
  </div>
  <!-- #menu end -->
  <!-- #page: holds the page content -->
  <div id="page">
    <!-- #columns: holds the columns of the page -->
    <div id="columns" class="widthPad">
      <!-- Left column -->
      <div class="floatLeft width100">
	    <br/>
		<h1> Random <span class="dark"> stuff</span></h1>
		<p> I had considered putting a blog in this space, but decided that
		that
		would be too structured and that I would be obligated to update it 
		regularly.
		I do occasionally have to urge
		to write something down or share a great recipe.  This page won't be
		updated regularly but it should fill with interesting tidbits
		eventually.  There will almost certainly not be any overarching theme,
		and I reserve the right to post about personal, culinary, philosophical,
		and highly technical (research
		related) issues.
		</p>
        <!-- -->
		<h1> <a name="memory"></a><span class="dark"> Use more memory than your machine
			  has</span></h1>
		<p> In performance computing, there are times when the size of the
		problem
		you can treat is limited not by computing speed, but by your
		machine's total memory.  This is the case for parts of the map making
		code I am developing.  For linear algebra, I'm using AMD graphics
		cards and the <i>acml-gpu</i> libraries to speed things up, and am now
		completely limited by the amount of memory on the machine.  I want to
		hold off working on a cluster as long as possible because waiting in a
		queue and transferring data back and forth from the cluster would slow
		development.  Just to put some numbers to things, the server I'm using
		has 144 GB of memory and 4 GPUs meaning I can deal with square matrices
		of order 100 000 on the side and each operation takes about an hour (in
		double precision).</p>
		
		<p> I have two tricks for stretching my memory further. This first is
		to use memory maps.  A memory map is essentially a file on disk that
		your machine treats as memory.  Now, your hard disk is a lot slower than
		main memory, but the beauty of a memory map is that the parts of the
		file that you are currently using get cached in memory, and so access
		and assignment are fast.  This works great if you have a
		lot of operations to perform at a time on a small part of a huge piece
		of data.  However, this requires you to be very careful.  If you access
		your data too sporadically, the program will grind to a halt.  Still
		I've found that memory maps are unbelievably forgiving, and tend to
		perform quite a bit better than you would initially expect.  The memory
		map functionality is available to python users through
		<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html"
		  ><i>numpy</i></a>, and is trivial to use.</p>

		<p> My second trick is much more subtle, and relies on modern operating
		systems being <a href="http://en.wikipedia.org/wiki/Virtual_memory"
		  >virtual memory systems</a>.  In a virtual memory system, physical
		memory is not allocated when a process allocates (mallocs) memory.
		Instead, virtual memory is.  Physical memory is only allocated when
		the virtual memory is used.  So when programming, you can feel free to
		allocate as much memory as you want, as long as you don't use it.  This
		is useful for me because the matrices I deal with are symmetric.  The
		linear algebra routines I'm using (standard <i>LAPACK</i> routines such
		as a Cholesky decomposition) never touch
		the lower triangular part of the matrix.  So, what I do is allocate a
		huge chunk of memory, only fill half of it, and have matrix operations
		performed in place.  On the machine I'm using, the maximum matrix size
		I can work with is 180 000 on each side, which takes about 6 hours.
		Not bad for not having to use a cluster.</p>

		<p> There is a slight complication in that <i>numpy</i> will not allow
		you to allocate more memory than your machine has (using the command
		np.empty).  I had to hack together some <i>cython</i> code so I could
		malloc the memory in <i>C</i> and convert it to a <i>numpy</i> array.
		I've posted the code below, feel free to use it, change it,
		whatever.</p>

		<script src="https://gist.github.com/1447078.js?file=large_empty.pyx"></script>
		<!-- -->
		<h1><a name=inverse></a><span class="dark"> The Binomial Inverse
			  theorem</span></h1>
		<p> Here is a matrix trick that I'm sure I wasn't taught in school.
		Let me prefix everything by saying that I'm currently working on a map
		making code for the 21 cm experiment I'm involved in.  Map making is
		big business in the CMB community and I'm trying to translate what is
		used there to 21 cm.  Map making is all about linear algebra and
		dealing with large matrices and one of the big challenges is inverting
		some of these matrices.</p>

		<p>The <a href="http://en.wikipedia.org/wiki/Binomial_inverse_theorem">binomial inverse theorem</a> 
		(a.k.a. the Woodbury matrix identity or
		the Sherman-Morrison-Woodbury formula) tells you how to invert sums of
		matrices for which you already know the inverses.  For a general pair of
		matrices it isn't that useful, since applying the formula is as much work
		as computing an inverse the normal way.  Where the theorem comes in
		handy is when one of the matrices is of low rank.  Lets say you have an 
		<i>(n*n)</i> matrix <i>A</i> for which you already have the inverse.
		You have another matrix <i>B</i> that is the same size but only has rank
		<i>m</i> which is a relatively small number.  The inverse of their sum
		can be computed in <i>O(m*n<sup>2</sup>)</i> operations instead of the
		normal <i>O(n<sup>3</sup>)</i>.  This is assuming you already know how
		to rotate <i>B</i> to an <i>(m*m)</i> dense matrix, but this is probably
		how you got <i>B</i> in the first place 
		(or else why would it have such a low rank?).</p>

		<p> All fine and good, but I still need to invert <i>A</i> so I haven't
		gained anything.  This brings us to where the theorem is <b>really</b>
		useful; if <i>A</i> of is full rank but is diagonal or block diagonal.
		This symmetry lets you invert and multiply by <i>A</i> quickly but
		does nothing for <i>A+B</i> unless you use binomial inverse theorem.
		This might seem like a pretty limited range of applicability but this
		situation has come up no less than 3 times in my map making code.</p>

		<p> An example is instructive.  Lets say you have a set of <i>n</i>
        random numbers arranged in a vector <i>d</i>.  Additionally, these
        numbers are uncorrelated, that is their covariance matrix
        <i>D</i> is diagonal, and <i>D<sup>-1</sup></i>
        is trivially computed.  Now lets
        say you want to add a highly correlated component <i>c = d + sv</i>,
        where <i>s</i> is a random number with variance <i>S</i>
        and <i>v</i> is some projection
        vector.  The new covariance matrix <i>C = D + vSv<sup>T</sup></i> is
        dense but its inverse can still be computed trivially:
        <i> C<sup>-1</sup> = D<sup>-1</sup> -
            D<sup>-1</sup>v(S<sup>-1</sup> +
            v<sup>T</sup>D<sup>-1</sup>v)<sup>-1</sup>v<sup>T</sup>D<sup>-1</sup>
		</i>.  This example is somewhat simplistic but is vaguely
		representative of the kind of situation that comes up all the time in
		cosmology.
		</p>

		<p> Now for the catch. For the most part the identity is numerically
		fairly stable.  However a problem arises when the update term (<i>S</i>
		in the above example) is much larger than the corresponding part of the
		original matrix (<i>v<sup>T</sup>Dv</i>).
		I will try to motivate the problem using the above example,
		but if your not into statistics you can skip to the bottom of the
		paragraph. First consider the quantities 
		<i>v<sup>T</sup>Dv</i> and <i>v<sup>T</sup>D<sup>-1</sup>v</i>.
		These are the variance and information for mode <i>v</i> before we
		added the extra variance <i>S</i>. Now when you add variance, you
		subtract information, that is, 
		<i> v<sup>T</sup>C<sup>-1</sup>v &lt S<sup>-1</sup>
		  &lt&lt v<sup>T</sup>D<sup>-1</sup>v</i>.  So here we
		see the problem.  If <i>S</i> is large, then the update going from 
		<i>D<sup>-1</sup> </i> to <i>C<sup>-1</sup> </i> needs to subtract out
		and nearly cancel all the information about mode <i>v</i>.  This is
		numerical disaster and tends to make your matrices cease to be
		positive definite.  Bottom line, the identity is numerically unstable if
		<i>S &gt&gt v<sup>T</sup>Dv </i> (if <i>v</i> isn't normalized you 
		need a factor of the normalization squared on the right hand side).
		The generalization for multiple
		modes (<i>v</i> a rectangular matrix instead of a vector) is obvious.
		Despite a pretty lengthy search for any
		reference or warning about using the identity, I had to learn this the
		hard way (weeks of debugging).
		</p>
		<!-- -->
      </div>
      <!-- Left end -->
    </div>
    <!-- #columns end -->
  </div>
  <!-- #page end -->
</div>
<!-- #content end -->
<!-- #footer: holds the site footer (logo and links) -->
<div id="footer">
  <!-- #bg: applies the site width and footer background -->
  <div id="bg" class="width">
	  <br/>
	  Kiyoshi Wesley Masui &nbsp;&nbsp; kiyo _at_ physics _dot_ ubc.ca &nbsp;&nbsp; tel: 647 761 3494
      &nbsp;&nbsp; office: <a href="http://www.phas.ubc.ca/contact-us">Hennings</a> 206
  </div>
  <!-- #bg end -->
</div>
<!-- #footer end -->
</body>
</html>
